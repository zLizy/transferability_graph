import pandas as pd
import numpy as np
import torch
import time
import os
from glob import glob

import xgboost as xgb
from sklearn.preprocessing import normalize
from sklearn.preprocessing import LabelEncoder,OneHotEncoder
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import MinMaxScaler
from sklearn import svm
from sklearn.ensemble import RandomForestRegressor


SAVE_FEATURE = True

class RegressionModel():
    def __init__(self,test_dataset,finetune_ratio=1.0,method='',hidden_channels=128,dataset_embed_method='domain_similarity',root='..',
                #  corr_path='corr_domain_similarity_google_vit_base_patch16_224_imagenet.csv',
                 SAVE_FEATURE=True):
        if dataset_embed_method == 'task2vec':
            corr_path = 'corr_task2vec_resnet50_imagenet.csv'
        elif dataset_embed_method == 'domain_similarity':
            corr_path = 'corr_domain_similarity_google_vit_base_patch16_224_imagenet.csv'
        self.selected_columns = ['input_shape', 'architectures','accuracy',
                            'model_type','number_of_parameters',
                            'elapsed_time', 'train_runtime',
                            'finetune_dataset','size', 'number_of_classes',
                            'dataset','#labels',
                            'test_accuracy'
                            ]
        dataset_map = {'oxford_iiit_pet':'pets',
                       'oxford_flowers102':'flowers'}
        if test_dataset in dataset_map.keys():
            self.test_dataset = dataset_map[test_dataset]
        else:
            self.test_dataset = test_dataset
        self.root = root
        self.finetune_ratio = finetune_ratio
        self.method = method
        self.corr_path = corr_path
        self.hidden_channels = hidden_channels

        if 'task2vec' in corr_path and 'task2vec' in dataset_embed_method:
            self.embed_addition = '_task2vec'
        else:
            self.embed_addition = ''
        
        if 'without_accuracy' in method:
            self.y_label = 'score'
        else:
            self.y_label = 'test_accuracy'
        pass

    def feature_preprocess(self,embedding_dict={},data_dict={}):

        df_model_config = pd.read_csv(os.path.join(self.root,'doc','model_config_dataset.csv'))
        # print(f'\n df_model_config.columns: {df_model_config.columns}')
        df_dataset_config = pd.read_csv(os.path.join(self.root,'doc','finetune_dataset_config.csv'))
        # print(f'\n df_dataset_config.columns: {df_dataset_config.columns}')
        
        df_finetune = pd.read_csv(os.path.join(self.root,'doc','records.csv'),index_col=0)
        df_finetune = df_finetune.rename(columns={'finetuned_dataset':'finetune_dataset'})
        df_finetune = df_finetune.drop(columns=['input_shape'])
        
        df_model = df_finetune.merge(df_model_config,how='inner',on='model')
    
        df_feature = df_model.merge(df_dataset_config,how='inner',on='finetune_dataset')
        
        df_feature = fill_null_value(df_feature,columns=['accuracy','number_of_parameters'])

        df_feature = df_feature.dropna(subset=['model_type'])
        
        if 'normalize' in self.method:
            df_feature['test_accuracy'] = df_feature[['finetune_dataset','test_accuracy']].groupby('finetune_dataset').transform(lambda x: (x - x.min()) / (x.max()-x.min()))
        
        
        if ('Conv' in self.method or 'node2vec' in self.method):
            if embedding_dict == {}:
                method = self.method
                if 'normalize' in method:
                    method = '_'.join(method.split('_')[:-1])
                if 'basic' in method or 'all' in method:
                    method = '_'.join(method.split('_')[:-1])
                # print(method)
                if method[-8:] == 'distance':
                    method = '_'.join(method.split('_')[:-2])

                root = f"../../features_final/{self.test_dataset}"
                if not os.path.exists(root):
                    os.makedirs(root)
                if self.finetune_ratio == 1:
                    file = os.path.join(f"{root}/features_{method.replace('rf_','lr_').replace('svm_','lr_').replace('xgb_','lr_')}_{self.hidden_channels}.csv")
                    print(f'\nfile: {file}')
                    # file = os.path.join(f"../../features/{self.test_dataset}/features_{method}.csv")
                    # if not os.path.exists(file):
                    # file = os.path.join(f"../../features/{self.test_dataset}/features_{method.replace('lr','rf').replace('svm','rf')}.csv")
                else:
                    # file = os.path.join(f"../../features/{self.test_dataset}/features_{method}_{self.finetune_ratio}.csv")
                    # if not os.path.exists(file):
                    file = os.path.join(f"{root}/features_{method.replace('rf_','lr_').replace('svm_','lr_').replace('xgb_','lr_')}_{self.hidden_channels}_{self.finetune_ratio}.csv")
                df_feature = pd.read_csv(file)#index_col=0
                print(f'\ndf_feature.shape: {df_feature.shape}')
                if 'model.1' in df_feature.columns:
                    # df_feature = df_feature.rename(columns={'model.1':'model'})
                    df_feature = df_feature.drop(columns=['model.1'])
                # print(df_feature.columns)

                df_feature.index = range(len(df_feature))

                df_feature = df_feature.dropna(subset=['model_type'])
                if 'normalize' in self.method:
                    df_feature['test_accuracy'] = df_feature[['finetune_dataset','test_accuracy']].groupby('finetune_dataset').transform(lambda x: (x - x.min()) / (x.max()-x.min()))
        
                
                # if 'basic' in self.method or 'all' in self.method or 'without_accuracy' in self.method:
                self.selected_columns += [col for col in df_feature.columns if 'm_f' in col or 'd_f' in col]
                # print()
                # print(list(df_feature.columns))

            if embedding_dict!={}:
                # assert data_dict == {}
                unique_model_id =  data_dict['unique_model_id']
                unique_model_id.index = range(len(unique_model_id))
                df_feature = df_feature.merge(unique_model_id,how='inner',on='model')
                df_data_id = data_dict['unique_dataset_id'].rename({'dataset':'finetune_dataset'},axis='columns')
                df_feature = df_feature.merge(df_data_id,how='inner',on='finetune_dataset')
                # print(f'\n df_feature.clumns: \n{df_feature.columns}')
                # print(f'\ndf_feature: {df_feature.head()}')
                ### Capture the embeddings
                df = pd.DataFrame()
                model_emb = []
                dataset_emb = []
                for i, row in df_feature.iterrows():
                    model_id = row['mappedID_x']
                    dataset_id = row['mappedID_y']
                    if 'node2vec' in self.method:
                        model_emb.append(embedding_dict[model_id].detach().numpy())
                        dataset_emb.append(embedding_dict[dataset_id].detach().numpy())
                    else:
                        # print(f'\nembedding_dict:{embedding_dict}')

                        model_emb.append(embedding_dict[model_id].numpy())
                        dataset_emb.append(embedding_dict[dataset_id].numpy())

                ## if 'all' in method, taking all the features into account
                if not 'all' in self.method: 
                    self.selected_columns = ['finetune_dataset',self.y_label]

                columns=['m_f'+str(i) for i in range(len(embedding_dict[model_id]))]
                df_ = pd.DataFrame(model_emb,columns=columns)
                # df_feature[columns] = model_emb
                df_feature = pd.concat([df_feature,df_],axis=1)
                self.selected_columns += columns
                
                columns=['d_f'+str(i) for i in range(len(embedding_dict[dataset_id]))]
                df_ = pd.DataFrame(dataset_emb,columns=columns)
                # df_feature[columns] = dataset_emb
                df_feature = pd.concat([df_feature,df_],axis=1)
                self.selected_columns += columns

                # print(df_feature.head())
                # df_feature.to_csv('./methods/features.csv')

                df_feature = df_feature.dropna(subset=['m_f0','d_f0'])
                # print(df_feature.head())

        if 'logme' in self.method or 'without_accuracy' in self.method: # or 'all' in self.method
            # print(df_feature.columns)
            # model_col_count = df_feature.columns.tolist().count('model')
            # if model_col_count > 1: df_feature = df_feature.drop(columns=['model'])
            
            if 'score' not in df_feature.columns:
                model_list = df_feature['model'].unique()
                # df_feature.index = df_feature['model']
                df_feature.index = range(len(df_feature))
                df_dataset_list = []
                for dataset in df_feature['finetune_dataset'].unique():
                    if embedding_dict == {}:
                        logme = pd.read_csv(f'../baselines/LogME_scores/{dataset}.csv',)
                    else:
                        logme = pd.read_csv(f'./baselines/LogME_scores/{dataset}.csv',)
                    df_logme = logme[logme['model']!='time']
                    # identify common models
                    # print('\n',df_feature['model'])
                    df_logme = df_logme[df_logme['model'].isin(model_list)]
                    df_logme = df_logme.dropna(subset=['score'])
                    # normalize
                    df_logme['score'].replace([-np.inf,np.nan], -50, inplace=True)
                    
                    score = df_logme['score'] #.astype('float64')
                    # normalized_pred = (score-score.min())/(score.max()-score.min()) #df_score['score'].values/norm 
                    normalized_pred = (score-score.mean())/score.std()
                    df_logme['score'] = normalized_pred
                    
                    # df_feature.loc[df_logme['model'].values,'score'] = normalized_pred
                    df_dataset_list.append(df_feature[df_feature['finetune_dataset']==dataset].merge(df_logme,how='inner',on=['model'])) #,'finetune_dataset']))
                df_feature = pd.concat(df_dataset_list)
                # print(f'after logme: if score in columns: {list(df_feature.columns)}')
            if 'score' not in self.selected_columns:
                self.selected_columns += ['score']
        

        if 'data_distance' in self.method or 'all' in self.method:
        # if True:
            corr_path = os.path.join(self.root,'doc',self.corr_path)
            df_corr = pd.read_csv(corr_path,index_col=0)
            columns = df_corr.columns
            df_corr['finetune_dataset'] = df_corr.index
            maps = {'oxford_iiit_pet':'pets', 'svhn_cropped':'svhn', 'oxford_flowers102':'flowers','smallnorb':'smallnorb_label_elevation'}
            for k, v in maps.items():
                df_corr = df_corr.replace(k,v)
            df_corr = pd.melt(df_corr,id_vars=['finetune_dataset'],var_name='dataset',value_vars=columns,value_name='distance')
            df_corr['distance'] = df_corr[['finetune_dataset','distance']].groupby('finetune_dataset').transform(lambda x: (x - x.min()) / (x.max()-x.min()))

            if 'distance' in df_feature.columns:
                df_feature = df_feature.drop(columns=['distance'])
            df_feature = df_feature.merge(df_corr,how='inner',on=['finetune_dataset','dataset'])
            # print(df_feature.head())
        
            self.selected_columns += ['distance']

        if 'feature' in self.method:
            print(f'feature in self.method')
            records = {'finetune_dataset':[],'model':[],'feature':[]}
            emb_files = glob(os.path.join('../../','model_embed','embeddings')+'/*')
            for file in emb_files:
                components = file.split(',')
                array = np.reshape(np.load(file),(1,-1))
                # print(f'array.shape: {array.shape}')
                array = normalize(array,norm='max').ravel()
                model = components[2].split('_')
                model_name = model[0] + '/' + '_'.join(model[1:])
                model_name = model_name.replace('.npy','')

                records['finetune_dataset'].append(components[1])
                records['model'].append(model_name)
                records['feature'].append(array)
            columns=['c'+str(i) for i in range(array.size)]
            self.selected_columns += columns
            df = pd.DataFrame.from_dict({k:v for k,v in records.items() if k != 'feature'})
            
            # Normalize features
            scaler = MinMaxScaler()
            features = records['feature']
            scaler.fit(features)
            featues = scaler.transform(features)
            df[columns] = featues
            # print(f'\ndf: {df.head()}')
            df_feature = df_feature.merge(df,how='inner',on=['finetune_dataset','model'])
            # print(f'\ndf_feature: {df_feature.head()}')
        
        # print(list(df_feature.columns))
        df_feature.index = df_feature['model']
        # print(f'\nSAVE_FEATURE: {SAVE_FEATURE}')
        if SAVE_FEATURE:
            # print(f'\n-----save feature')
            _dir = os.path.join(self.root,'features_final',self.test_dataset)
            if not os.path.exists(_dir):
                os.makedirs(_dir)
            if self.finetune_ratio < 1:
                file = os.path.join(_dir,f'features_{self.method}{self.embed_addition}_{self.hidden_channels}_{self.finetune_ratio}.csv')
            else:
                file = os.path.join(_dir,f'features_{self.method}{self.embed_addition}_{self.hidden_channels}.csv')
            # if not os.path.exists(file):
            #     df_feature.to_csv(file)
            print(df_feature.shape)
            df_feature.to_csv(file)

        # print(f'\n selected_columns: {self.selected_columns}')
        df_feature = df_feature[self.selected_columns]
        if 'score' in df_feature.columns:
            df_feature = df_feature.dropna(subset=['score'])
        print(f'\n df_feature.len: {len(df_feature)}')

        nan_columns = df_feature.columns[df_feature.isna().any()].tolist()
        print(f'nan_columns: {nan_columns}')
        df_feature = fill_null_value(df_feature,columns=nan_columns,value=0)
        # print(f'df_feature.columns:{df_feature.columns}')
        # print(df_feature[['finetune_dataset','score']].head())

                                                                      
        self.df_feature = df_feature

        # 
        # print(f'\n nan_columns: {nan_columns}')
        # normal_columns = [col for col in df_feature.columns if 'm_f' not in col and 'd_f' not in col]
        # df_feature[normal_columns].to_csv('../../features/features.csv')
    
    def split(self):

        df_train = self.df_feature[self.df_feature['finetune_dataset']!=self.test_dataset]

        ##### Sampling the finetune records given the ratio
        if self.finetune_ratio != 1:
            df_train = df_train.sample(frac=self.finetune_ratio, random_state=1)

        # df_train = df_train.drop(columns='finetune_dataset')
        df_test = self.df_feature[self.df_feature['finetune_dataset']==self.test_dataset]
        # print(f'\n - test_dataset: {self.test_dataset}')
        # print(df_test.head())

        # df_test = df_test.drop(columns='finetune_dataset')
        self.selected_columns.remove('finetune_dataset')


        categorical_columns = ['architectures','model_type','finetune_dataset','dataset']
        if set(categorical_columns) < set(list(df_train.columns)):
            df_train = encode(df_train,categorical_columns)
            df_test = encode(df_test,categorical_columns)

        if 0:
            df_train.to_csv(os.path.join(self.root,'doc','train.csv'))
            df_test.to_csv(os.path.join(self.root,'doc','test.csv'))
        
        return df_train, df_test
        
    def train(self,embedding_dict={},data_dict={},save_path='',run=0):
        # if 'Conv' in method:
        #     method = 'lr_' + method
        # print(embedding_dict)

        self.feature_preprocess(embedding_dict,data_dict)

        df_train, df_test = self.split()

        feature_columns = [col for col in self.selected_columns if col != self.y_label]
        X_train = df_train[feature_columns].values
        X_test = df_test[feature_columns].values
        y_train = df_train[self.y_label].values
        y_test = df_test[self.y_label].values

        print(f'\n --- shape of X_train: {X_train.shape}, X_test.shape: {X_test.shape}')
        # print(f'X_test:{X_test}')
        # ### Model
        if 'lr' in self.method:
            model = LinearRegression()    
        elif 'rf' in self.method:
            model = RandomForestRegressor(n_estimators = 100, 
                                          max_features = 'sqrt',
                                          max_depth = 5, 
                                          random_state = 18)
        elif 'svm' in self.method:
            model = svm.SVR()
        elif 'xgb' in self.method:
            model = xgb.XGBRegressor(n_estimators=500, max_depth=5, eta=0.1,subsample=1, colsample_bytree=0.8,random_state=18) #random_state=18,subsample=0.7,
            # early_stop = xgb.callback.EarlyStopping(
            #     rounds=2, metric_name='logloss', data_name='Validation_0', save_best=True
            # )
            # model = xgb.XGBClassifier(tree_method="hist") #callbacks=[early_stop] early_stopping_rounds=2

        model.fit(X_train,y_train)
        score = model.score(X_test,y_test)
        

        print(f'\n === score: {score}')
        y_pred = model.predict(X_test)
        # print(f'\n y_pred: {y_pred[:10]}')

        df_results = pd.DataFrame({'model':df_test.index,'score':y_pred})
        
        if embedding_dict == {}:
            dir_path = os.path.join('../rank_final',f'{self.test_dataset}', self.method)
        else:
            dir_path = os.path.join('./rank_final',f'{self.test_dataset}', self.method)
        if not os.path.exists(dir_path):
            os.makedirs(dir_path)
        if self.finetune_ratio > 1:
            file = f'results{self.embed_addition}_{self.hidden_channels}_{run}.csv'
        else:
            file = f'results{self.embed_addition}_{self.finetune_ratio}_{self.hidden_channels}_{run}.csv'
        
        print(f'\n -- os.path.join(dir_path,file): {os.path.join(dir_path,file)}')
        df_results.to_csv(os.path.join(dir_path,file))

        return score, df_results

def fill_null_value(df,columns,value='mean'):
    for col in columns:
        if value == 'mean':
            df[col].fillna((df[col].mean()), inplace=True)
        else:
            df[col].fillna((value), inplace=True)
    return df

def encode(df,columns):
    # convert features to one-hot encoding
    encoder = LabelEncoder()
    for col in columns:
        # print(f'\n encoder col:{col}')
        _df = pd.DataFrame(encoder.fit_transform(df[col]),columns=[col])
        _df.index = df.index
        df = df.drop(columns=[col])
        df = pd.concat([df,_df],axis=1 )
    return df

if __name__ == '__main__':
    
    path = os.path.join('../../','doc',f'performance_rf_score.csv')
    print(f'====== path: {path} ======')
    if os.path.exists(path):
        df_perf = pd.read_csv(path,index_col=0)
    else:
        df_perf = pd.DataFrame(columns=[
                        'method',
                        'finetune_dataset',
                        'train_time',
                        'score'
        ])

    datasets = [
                            #  'diabetic_retinopathy_detection',
                            #  'kitti',
                            #  'svhn',
                            #  'smallnorb_label_azimuth',
                            #  'smallnorb_label_elevation',
                            #  'stanfordcars',
                            #  'cifar100',
                            #  'caltech101',
                            #  'dtd',
                            #  'eurosat',
                             'oxford_flowers102',
                             'oxford_iiit_pet',
                            #  'pets','flowers',
                ]

    # ratios = 0.3,0.5,0.7: lr_node2vec+ lr_homo_SAGEConv
    for method in [
                    # 'lr_normalize','rf_normalize','svm_normalize',
                    # 'svm_node2vec_normalize',
                    # 'svm_node2vec+_normalize',
                    # 'svm_data_distance_normalize',
                    # 'svm_node2vec+_normalize',
                    # 'svm_logme_data_distance_normalize',
                    # 'svm_node2vec_basic_normalize',
                    # 'svm_node2vec_all_normalize',
                    # 'svm_node2vec+_basic_normalize',
                    # 'svm_node2vec+_all_normalize',
                    # 'svm_node2vec_without_accuracy_all_normalize', 
                    # 'svm_node2vec+_without_accuracy_all_normalize',
                    # 'svm_node2vec_without_accuracy_basic_normalize', 
                    # 'svm_node2vec+_without_accuracy_basic_normalize',
                    # 'svm_node2vec_without_accuracy_normalize', 
                    # 'svm_node2vec+_without_accuracy_normalize',
                    # 'svm_homo_SAGEConv_trained_on_transfer',
                    # 'svm_homo_SAGEConv_trained_on_transfer_basic',
                    # 'xgb_normalize',
                    # 'xgb_data_distance_normalize',
                    # 'xgb_logme_data_distance_normalize',
                    # 'xgb_node2vec_basic_normalize', 
                    # 'xgb_node2vec+_basic_normalize',
                    # 'xgb_node2vec_without_accuracy_all_normalize', 
                    # 'xgb_node2vec+_without_accuracy_all_normalize',
                    # 'xgb_node2vec_without_accuracy_basic_normalize', 
                    # 'xgb_node2vec+_without_accuracy_basic_normalize',
                    # 'xgb_node2vec_without_accuracy_normalize', 
                    # 'xgb_node2vec+_without_accuracy_normalize',
                    # 'xgb_node2vec_all_normalize', 
                    # 'xgb_node2vec+_all_normalize',
                    # 'xgb_node2vec_normalize', 
                    # 'xgb_node2vec+_normalize',
                    # 'xgb_node2vec_data_distance_normalize',
                    # 'xgb_node2vec+_data_distance_normalize',

                    # 'rf_data_distance_normalize',
                    # 'rf_logme_data_distance_normalize',
                    # 'rf_node2vec_normalize', 
                    # 'rf_node2vec+_normalize',
                    # 'rf_node2vec_basic_normalize', 
                    # 'rf_node2vec+_basic_normalize',
                    # 'rf_node2vec_all_normalize', 
                    # 'rf_node2vec+_all_normalize',
                    # 'rf_node2vec_data_distance_normalize',
                    # 'rf_node2vec+_data_distance_normalize',
                    # 'rf_node2vec_without_accuracy_all_normalize', 
                    # 'rf_node2vec+_without_accuracy_all_normalize',
                    # 'rf_node2vec_without_accuracy_basic_normalize', 
                    # 'rf_node2vec+_without_accuracy_basic_normalize',
                    # 'rf_node2vec_without_accuracy_normalize', 
                    # 'rf_node2vec+_without_accuracy_normalize',

                    # 'lr_data_distance_normalize',
                    # 'lr_logme_data_distance_normalize',
                    # 'lr_node2vec_normalize', 
                    # 'lr_node2vec+_normalize',
                    # 'lr_node2vec_all_normalize', 
                    # 'lr_node2vec+_all_normalize',
                    # 'lr_node2vec_basic_normalize', 
                    # 'lr_node2vec+_basic_normalize',
                    # 'lr_node2vec_data_distance_normalize',
                    # 'lr_node2vec+_data_distance_normalize',
                    # 'lr_node2vec_without_accuracy_all_normalize', 
                    # 'lr_node2vec+_without_accuracy_all_normalize',
                    # 'lr_node2vec_without_accuracy_basic_normalize', 
                    # 'lr_node2vec+_without_accuracy_basic_normalize',
                    # 'lr_node2vec_without_accuracy_normalize', 
                    # 'lr_node2vec+_without_accuracy_normalize',

                    'lr_homo_SAGEConv_normalize',
                    'lr_homoGATConv_normalize',
                    'rf_homo_SAGEConv_normalize',
                    'rf_homoGATConv_normalize',
                    'xgb_homo_SAGEConv_normalize',
                    'xgb_homoGATConv_normalize',
                    
                    'lr_homo_SAGEConv_basic_normalize',
                    'lr_homoGATConv_basic_normalize',
                    'rf_homo_SAGEConv_basic_normalize',
                    'rf_homoGATConv_basic_normalize',
                    'xgb_homo_SAGEConv_basic_normalize',
                    'xgb_homoGATConv_basic_normalize',

                    'lr_homo_SAGEConv_without_accuracy_basic_normalize',
                    'lr_homoGATConv_without_accuracy_basic_normalize',
                    'rf_homo_SAGEConv_without_accuracy_basic_normalize',
                    'rf_homoGATConv_without_accuracy_basic_normalize',
                    'xgb_homo_SAGEConv_without_accuracy_basic_normalize',
                    'xgb_homoGATConv_without_accuracy_basic_normalize',

                    'rf_homoGCNConv_normalize',
                    'xgb_homoGCNConv_normalize',
                    'lr_homoGCNConv_normalize',
                    'xgb_homoGCNConv_basic_normalize',
                    'rf_homoGCNConv_basic_normalize',
                    'lr_homoGCNConv_basic_normalize',
                    'lr_homoGCNConv_all_normalize',
                    'rf_homoGCNConv_all_normalize',
                    'xgb_homoGCNConv_all_normalize',

                    'lr_homo_SAGEConv_all_normalize',
                    'lr_homoGATConv_all_normalize',
                    'rf_homo_SAGEConv_all_normalize',
                    'rf_homoGATConv_all_normalize',
                    'xgb_homo_SAGEConv_all_normalize',
                    'xgb_homoGATConv_all_normalize',

                    'lr_homo_SAGEConv_without_accuracy_all_normalize',
                    'lr_homoGATConv_without_accuracy_all_normalize',
                    'rf_homo_SAGEConv_without_accuracy_all_normalize',
                    'rf_homoGATConv_without_accuracy_all_normalize',
                    'xgb_homo_SAGEConv_without_accuracy_all_normalize',
                    'xgb_homoGATConv_without_accuracy_all_normalize',
                    
                   ]:

        for test_dataset in datasets: 
            print(f'======== test_dataset: {test_dataset}, method: {method} =============')
            for ratio in [1.0]: #: 0.6, 0.8   0.3, 0.5, 0.7
                print(f'\n -- ratio: {ratio}')
                start = time.time()
                df_list = []
                for hidden_channels in [128]: #32,64,
                    print (f'\n -------- hidden_channels: {hidden_channels}')
                    trainer = RegressionModel(test_dataset,
                                            finetune_ratio=ratio,
                                            method=method,
                                            hidden_channels=hidden_channels,
                                            root='../../',
                                            dataset_embed_method= 'domain_similarity' #'', #  task2vec
                                            )
                    # try:
                    score, df_results = trainer.train()
                    # except Exception as e:
                    #     print(e)
                    #     continue
                train_time = time.time() - start
                df_perf.loc[len(df_perf)] = [method, test_dataset,train_time,score]
                

        df_perf.to_csv(path)